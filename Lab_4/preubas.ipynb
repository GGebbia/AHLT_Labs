{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTIONS ####\n",
    "\n",
    "def analyze(s):\n",
    "    mytree = my_parser.raw_parse(s)\n",
    "    last_offset_end = 0\n",
    "    for head_node in mytree:\n",
    "        print(list(head_node.nodes.viewkeys())[1:])\n",
    "        for key in sorted(head_node.nodes, key=lambda key: int(key)):\n",
    "            # first key is not first word (is root)\n",
    "            if key == 0:\n",
    "                continue\n",
    "            # find first occurrence of substring token in sentence\n",
    "            word = head_node.nodes[key]['word']\n",
    "            offset_start = s.find(word, last_offset_end)\n",
    "            offset_end = offset_start + len(word) - 1  # -1 as length 1 is same start and end\n",
    "            # store last offsets\n",
    "            last_offset_end = offset_end\n",
    "            # add start and end to the token\n",
    "            head_node.nodes[key]['start'] = offset_start\n",
    "            head_node.nodes[key]['end'] = offset_end\n",
    "\n",
    "    return head_node\n",
    "\n",
    "# Returns the key corresponding to the entity in the analysis dependency graph\n",
    "def get_entity_node_key(entity, analysis):\n",
    "    for key in sorted(analysis.nodes, key=lambda key: int(key)):\n",
    "        try:\n",
    "            if analysis.nodes[key]['start'] == int(entity[0]):\n",
    "                return key\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "# Checks if the input word (belonging to a node) is a parent of the input parent\n",
    "def isNodeInParent(parent, word):\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == word:\n",
    "                return True\n",
    "            if isNodeInParent(node, word):\n",
    "                return True\n",
    "        else:\n",
    "            if node == word:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# receives DependencyGraph with all sentence, list of entities and the ids of the 2 entities to be checked\n",
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    #### RULE VARIABLES ####\n",
    "    effect_clue_words = {\"administer\", \"potentiate\", \"prevent\", \"antagonize\", \"antagonized\"}\n",
    "    mechanism_clue_words = {\"reduce\", \"increase\", \"decrease\"}\n",
    "    int_clue_words = {\"interact\", \"interaction\"}\n",
    "    advise_clue_words = {\"may\", \"might\", \"should\"}\n",
    "\n",
    "    # results and interaction type, 0 and null unless we find some\n",
    "    result = \"0\"\n",
    "    interaction = \"null\"\n",
    "\n",
    "    # key of the node of entity 1 and entity 2\n",
    "    e1_node_key = get_entity_node_key(entities[e1], analysis)\n",
    "    e2_node_key = get_entity_node_key(entities[e2], analysis)\n",
    "    # get the corresponding word (token) of each entity from the dependency graph\n",
    "    e1_word = analysis.nodes[e1_node_key]['word']\n",
    "    e2_word = analysis.nodes[e2_node_key]['word']\n",
    "\n",
    "    # if not found assume no interaction\n",
    "    if e1_node_key == 0 or e2_node_key == 0:\n",
    "        return result, interaction\n",
    "\n",
    "    # get tree of dependency graph, will be used to check hierarchy (words under some others)\n",
    "    tree = analysis.tree()\n",
    "\n",
    "    # Iterate through all nodes in the graph\n",
    "    for key in sorted(analysis.nodes, key=lambda key: int(key)):\n",
    "        try:\n",
    "            # get current word of current node\n",
    "            current_word = analysis.nodes[key]['word']\n",
    "\n",
    "            # Check if interaction is advise\n",
    "            if current_word in advise_clue_words:\n",
    "                next_word = analysis.nodes[key + 1]['word']\n",
    "                if next_word == \"not\" and analysis.nodes[key + 2]['word'] == \"be\" \\\n",
    "                        and analysis.nodes[key + 3]['tag'] == \"VBN\" \\\n",
    "                        or next_word == \"be\" and analysis.nodes[key + 2]['tag'] == \"VBN\":\n",
    "                    result = \"1\"\n",
    "                    interaction = \"advise\"\n",
    "\n",
    "            # Check if interaction is int\n",
    "            elif current_word in int_clue_words:\n",
    "                for subtree in tree.subtrees():\n",
    "                    if subtree.label() in int_clue_words:\n",
    "                        if isNodeInParent(subtree, e1_word) and isNodeInParent(subtree, e2_word):\n",
    "                            result = \"1\"\n",
    "                            interaction = \"int\"\n",
    "\n",
    "            # Check if interaction is effect\n",
    "            elif current_word in effect_clue_words:\n",
    "                next_word = analysis.nodes[key + 1]['word']\n",
    "                # Explicit observed structure\n",
    "                if current_word == \"antagonize\" and next_word == \"the\":\n",
    "                    if analysis.nodes[key+2]['word'] == e1_word or analysis.nodes[key+2]['word'] == e2_word:\n",
    "                        result = \"1\"\n",
    "                        interaction = \"effect\"\n",
    "                elif current_word == \"antagonized\" and next_word == \"by\":\n",
    "                    if analysis.nodes[key + 2]['word'] == e1_word or analysis.nodes[key + 2]['word'] == e2_word:\n",
    "                        result = \"1\"\n",
    "                        interaction = \"effect\"\n",
    "                # Generic structure (entities are childs of clue word)\n",
    "                else:\n",
    "                    for subtree in tree.subtrees():\n",
    "                        if subtree.label() in effect_clue_words:\n",
    "                            if isNodeInParent(subtree, e1_word) and isNodeInParent(subtree, e2_word):\n",
    "                                result = \"1\"\n",
    "                                interaction = \"effect\"\n",
    "\n",
    "            # Check if interaction is mechanism\n",
    "            elif current_word in mechanism_clue_words:\n",
    "                for subtree in tree.subtrees():\n",
    "                    if subtree.label() in mechanism_clue_words:\n",
    "                        if isNodeInParent(subtree, e1_word) and isNodeInParent(subtree, e2_word):\n",
    "                            result = \"1\"\n",
    "                            interaction = \"mechanism\"\n",
    "\n",
    "        # If there's a KeyError, which can happen if there's extra non-numeric keys at the end of the tree (we are not \\\n",
    "        # interested in them, pass\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return result, interaction\n",
    "\n",
    "\n",
    "# receives data dir and filename for the results to evaluate\n",
    "def evaluate(inputdir, outputfile):\n",
    "    os.system(\"java -jar eval/evaluateDDI.jar \" + inputdir + \" \" + outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### VARIABLES ####\n",
    "inputdir = \"data/Devel\"\n",
    "outputfilename = \"./task9.2_TrainGianMarc_1.txt\"\n",
    "outputfile = open(outputfilename, \"w\")\n",
    "\n",
    "# connect to CoreNLP server\n",
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "#### MAIN ####\n",
    "\n",
    "# TODO mirar ordre!\n",
    "\n",
    "# process each file in directory\n",
    "for filename in os.listdir(inputdir):\n",
    "    # parse XML file, obtaining a DOM tree\n",
    "    file_path = os.path.join(inputdir, filename)\n",
    "    tree = ET.parse(file_path)\n",
    "    sentences = tree.findall(\"sentence\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "        (sid, stext) = (sentence.attrib[\"id\"], sentence.attrib[\"text\"])\n",
    "\n",
    "        # load sentence entities into a dictionary\n",
    "        entities = {}\n",
    "        ents = sentence.findall(\"entity\")\n",
    "        for e in ents:\n",
    "            ent_id = e.attrib[\"id\"]\n",
    "            offs = e.attrib[\"charOffset\"].split(\"-\")\n",
    "            entities[ent_id] = offs\n",
    "\n",
    "        # Tokenize, tag, and parse sentence\n",
    "        if not stext:\n",
    "            continue\n",
    "        analysis = analyze(stext)\n",
    "        # for each pair in the sentence, decide whether it is DDI and its type\n",
    "        pairs = sentence.findall(\"pair\")\n",
    "        for pair in pairs:\n",
    "            id_e1 = pair.attrib[\"e1\"]\n",
    "            id_e2 = pair.attrib[\"e2\"]\n",
    "            (is_ddi, ddi_type) = check_interaction(analysis, entities, id_e1, id_e2)\n",
    "            line = \"|\".join([sid, id_e1, id_e2, is_ddi, ddi_type])\n",
    "            outputfile.write(line + \"\\n\")\n",
    "\n",
    "evaluate(inputdir, outputfilename)\n",
    "outputfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "\n",
    "s  = \"Hello how are you paracetamol?\"\n",
    "mytree,  = my_parser.raw_parse(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = mytree.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycrfsuite\n",
    "import argparse\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"megam.dat\"\n",
    "test_filename = \"megam.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = \"predicted.txt\"\n",
    "model_filename = \"model.crfsuite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = open(train_filename, \"r\").read().split(\"\\n\")\n",
    "test_samples = open(test_filename, \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features):\n",
    "    \"\"\"\n",
    "    Receive a list of featured sentences splitted by words and split it into samples and labels.\n",
    "\n",
    "    Parameters:\n",
    "    word_features(list): List of feature words. There is an empty element between sentences in order to split each one.\n",
    "\n",
    "    Returns:\n",
    "    X_samples(list): List of feature words missing the label of the word.\n",
    "    Y_labels(list): List of labels for each word.\n",
    "    \"\"\"\n",
    "\n",
    "    X_samples = []\n",
    "    Y_labels = []\n",
    "\n",
    "    for feat in features:\n",
    "        feat = feat.split(\" \")\n",
    "        Y_labels.append(feat[0])\n",
    "        X_samples.append(feat[1:])\n",
    "\n",
    "    return X_samples, Y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = split_data(train_samples)\n",
    "X_test, Y_test = split_data(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The numbers of items and labels differ: |x| = 1, |y| = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-7b2eac482be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m trainer.set_params({\n",
      "\u001b[0;32mpycrfsuite/_pycrfsuite.pyx\u001b[0m in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The numbers of items and labels differ: |x| = 1, |y| = 4"
     ]
    }
   ],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(X_train, Y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "\n",
    "trainer.set_params({\n",
    "    'c1': 0.05,  # coefficient for L1 penalty\n",
    "    'c2': 0.1,  # coefficient for L2 penalty 1e-1 0.61\n",
    "    'max_iterations': 10000,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "\n",
    "trainer.train(model_filename)\n",
    "\n",
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open(model_filename)\n",
    "\n",
    "Y_pred = [tagger.tag(xseq) for xseq in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:                                                                                                                                                                           \n",
    "                                                                                                                                                                                         \n",
    "    def __init__(self, **kwargs):                                                                                                                                                       \n",
    "        self.word = kwargs[\"text\"]                                                                                                                                                      \n",
    "        self.offset_from, self.offset_to = self.parse_offset(kwargs[\"charOffset\"])                                                                                                      \n",
    "        self.id = kwargs[\"id\"]                                                                                                                                                          \n",
    "\n",
    "    def parse_offset(self, offset):                                                                                                                                                     \n",
    "\n",
    "        # offset can be given in two ways                                                                                                                                               \n",
    "        # e.g.:                                                                                                                                                                         \n",
    "        #       * 9-23                                                                                                                                                                  \n",
    "        #       * 9-11;12-20;21-23                                                                                                                                                      \n",
    "        #                                                                                                                                                                               \n",
    "        # We differenciate both cases and always save the first one and the last one                                                                                                    \n",
    "\n",
    "        if \";\" in offset:                                                                                                                                                               \n",
    "            offset = offset.split(\";\")                                                                                                                                                  \n",
    "            offset_from = offset[0].split('-')[0]                                                                                                                                       \n",
    "            offset_to = offset[-1].split('-')[1]                                                                                                                                        \n",
    "        else:                                                                                                                                                                           \n",
    "            offset_from, offset_to = offset.split('-')                                                                                                                                  \n",
    "\n",
    "        return int(offset_from), int(offset_to)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")\n",
    "mytree,  = my_parser.raw_parse(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "analysis = analyze(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caution should be exercised when combining resorcinol or salicylic acid with DIFFERIN Gel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Caution should be exercised when combining resorcinol or salicylic acid with DIFFERIN Gel\"\n",
    "e1 = \"resorcinol\"\n",
    "e2 = \"salicylic acid\"\n",
    "entities = [Entity(**{\"text\": e1, \"charOffset\": \"21-30\", \"id\": \"DDI-DrugBank.d200.s0.e0\"}), Entity(**{\"text\": e2, \"charOffset\": \"37-45\", \"id\": \"DDI-DrugBank.d200.s0.e1\"})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "analysis = analyze(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function nltk.parse.dependencygraph.<lambda>>,\n",
       "            {0: {u'address': 0,\n",
       "              u'ctag': u'TOP',\n",
       "              u'deps': defaultdict(list, {u'ROOT': [4]}),\n",
       "              u'feats': None,\n",
       "              u'head': None,\n",
       "              u'lemma': None,\n",
       "              u'rel': None,\n",
       "              u'tag': u'TOP',\n",
       "              u'word': None},\n",
       "             1: {u'address': 1,\n",
       "              u'ctag': u'NN',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 6,\n",
       "              u'feats': u'_',\n",
       "              u'head': 4,\n",
       "              u'lemma': u'caution',\n",
       "              u'rel': u'nsubjpass',\n",
       "              'start': 0,\n",
       "              u'tag': u'NN',\n",
       "              u'word': u'Caution'},\n",
       "             2: {u'address': 2,\n",
       "              u'ctag': u'MD',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 13,\n",
       "              u'feats': u'_',\n",
       "              u'head': 4,\n",
       "              u'lemma': u'should',\n",
       "              u'rel': u'aux',\n",
       "              'start': 8,\n",
       "              u'tag': u'MD',\n",
       "              u'word': u'should'},\n",
       "             3: {u'address': 3,\n",
       "              u'ctag': u'VB',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 16,\n",
       "              u'feats': u'_',\n",
       "              u'head': 4,\n",
       "              u'lemma': u'be',\n",
       "              u'rel': u'auxpass',\n",
       "              'start': 15,\n",
       "              u'tag': u'VB',\n",
       "              u'word': u'be'},\n",
       "             4: {u'address': 4,\n",
       "              u'ctag': u'VBN',\n",
       "              u'deps': defaultdict(list,\n",
       "                          {u'advcl': [6],\n",
       "                           u'aux': [2],\n",
       "                           u'auxpass': [3],\n",
       "                           u'nsubjpass': [1]}),\n",
       "              'end': 26,\n",
       "              u'feats': u'_',\n",
       "              u'head': 0,\n",
       "              u'lemma': u'exercise',\n",
       "              u'rel': u'ROOT',\n",
       "              'start': 18,\n",
       "              u'tag': u'VBN',\n",
       "              u'word': u'exercised'},\n",
       "             5: {u'address': 5,\n",
       "              u'ctag': u'WRB',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 31,\n",
       "              u'feats': u'_',\n",
       "              u'head': 6,\n",
       "              u'lemma': u'when',\n",
       "              u'rel': u'advmod',\n",
       "              'start': 28,\n",
       "              u'tag': u'WRB',\n",
       "              u'word': u'when'},\n",
       "             6: {u'address': 6,\n",
       "              u'ctag': u'VBG',\n",
       "              u'deps': defaultdict(list,\n",
       "                          {u'advmod': [5], u'dobj': [7], u'nmod': [13]}),\n",
       "              'end': 41,\n",
       "              u'feats': u'_',\n",
       "              u'head': 4,\n",
       "              u'lemma': u'combine',\n",
       "              u'rel': u'advcl',\n",
       "              'start': 33,\n",
       "              u'tag': u'VBG',\n",
       "              u'word': u'combining'},\n",
       "             7: {u'address': 7,\n",
       "              u'ctag': u'NN',\n",
       "              u'deps': defaultdict(list, {u'cc': [8], u'conj': [10]}),\n",
       "              'end': 52,\n",
       "              u'feats': u'_',\n",
       "              u'head': 6,\n",
       "              u'lemma': u'resorcinol',\n",
       "              u'rel': u'dobj',\n",
       "              'start': 43,\n",
       "              u'tag': u'NN',\n",
       "              u'word': u'resorcinol'},\n",
       "             8: {u'address': 8,\n",
       "              u'ctag': u'CC',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 55,\n",
       "              u'feats': u'_',\n",
       "              u'head': 7,\n",
       "              u'lemma': u'or',\n",
       "              u'rel': u'cc',\n",
       "              'start': 54,\n",
       "              u'tag': u'CC',\n",
       "              u'word': u'or'},\n",
       "             9: {u'address': 9,\n",
       "              u'ctag': u'JJ',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 65,\n",
       "              u'feats': u'_',\n",
       "              u'head': 10,\n",
       "              u'lemma': u'salicylic',\n",
       "              u'rel': u'amod',\n",
       "              'start': 57,\n",
       "              u'tag': u'JJ',\n",
       "              u'word': u'salicylic'},\n",
       "             10: {u'address': 10,\n",
       "              u'ctag': u'NN',\n",
       "              u'deps': defaultdict(list, {u'amod': [9]}),\n",
       "              'end': 70,\n",
       "              u'feats': u'_',\n",
       "              u'head': 7,\n",
       "              u'lemma': u'acid',\n",
       "              u'rel': u'conj',\n",
       "              'start': 67,\n",
       "              u'tag': u'NN',\n",
       "              u'word': u'acid'},\n",
       "             11: {u'address': 11,\n",
       "              u'ctag': u'IN',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 75,\n",
       "              u'feats': u'_',\n",
       "              u'head': 13,\n",
       "              u'lemma': u'with',\n",
       "              u'rel': u'case',\n",
       "              'start': 72,\n",
       "              u'tag': u'IN',\n",
       "              u'word': u'with'},\n",
       "             12: {u'address': 12,\n",
       "              u'ctag': u'NNP',\n",
       "              u'deps': defaultdict(list, {}),\n",
       "              'end': 84,\n",
       "              u'feats': u'_',\n",
       "              u'head': 13,\n",
       "              u'lemma': u'DIFFERIN',\n",
       "              u'rel': u'compound',\n",
       "              'start': 77,\n",
       "              u'tag': u'NNP',\n",
       "              u'word': u'DIFFERIN'},\n",
       "             13: {u'address': 13,\n",
       "              u'ctag': u'NN',\n",
       "              u'deps': defaultdict(list, {u'case': [11], u'compound': [12]}),\n",
       "              'end': 88,\n",
       "              u'feats': u'_',\n",
       "              u'head': 6,\n",
       "              u'lemma': u'gel',\n",
       "              u'rel': u'nmod',\n",
       "              'start': 86,\n",
       "              u'tag': u'NN',\n",
       "              u'word': u'Gel'}})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-administration\tNN\t8\tnsubjpass\n",
      "of\tIN\t3\tcase\n",
      "probenecid\tNN\t1\tnmod\n",
      "with\tIN\t5\tcase\n",
      "acyclovir\tNN\t1\tnmod\n",
      "has\tVBZ\t8\taux\n",
      "been\tVBN\t8\tauxpass\n",
      "shown\tVBN\t0\tROOT\n",
      "to\tTO\t10\tmark\n",
      "increase\tVB\t8\txcomp\n",
      "the\tDT\t13\tdet\n",
      "mean\tNN\t13\tcompound\n",
      "half-life\tNN\t10\tdobj\n",
      "and\tCC\t13\tcc\n",
      "the\tDT\t16\tdet\n",
      "area\tNN\t13\tconj\n",
      "under\tIN\t20\tcase\n",
      "the\tDT\t20\tdet\n",
      "concentration-time\tJJ\t20\tamod\n",
      "curve\tNN\t10\tnmod\n",
      ".\t.\t8\tpunct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(analysis.to_conll(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
