{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ggebbia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ggebbia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, span_generator):\n",
    "    return [(sentence[span[0]: span[1]], span[0], span[1]-1) for span in span_generator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(tokenized_list):\n",
    "    entities_list = []\n",
    "    last_type = None\n",
    "    word_stripped = False\n",
    "    list_length = len(tokenized_list)-1\n",
    "    for i, token in enumerate(tokenized_list):\n",
    "        word, offset_from, offset_to = token\n",
    "        d = {}\n",
    "        \n",
    "        # remove . from last word \n",
    "        if index == list_length and word.endswith(\".\"):\n",
    "            word = word[:-1]\n",
    "        # strip , and : from word\n",
    "        elif (word.endswith(\":\") or word.endswith(\",\")):\n",
    "            word = word[:-1]\n",
    "            word_stripped = True\n",
    "        \n",
    "        \n",
    "        if (word[0].isupper() and offset_from != 0) or (word.isupper()) or (word.lower()==\"spermine\"):\n",
    "            if last_type != None:\n",
    "                prev_word, prev_offset_from, _ = tokenized_list[i-1]\n",
    "                entities_list.pop(-1) \n",
    "                d[\"name\"] = prev_word + \" acid\"\n",
    "                d[\"type\"] = \"drug\"\n",
    "                d[\"offset\"] = \"{}-{}\".format(prev_offset_from, offset_to)\n",
    "            else:\n",
    "                d[\"name\"] = word\n",
    "                d[\"type\"] = \"drug\" # Posar pes per fer probabilitat 2/3 si es drug, 1/3 si es brand\n",
    "                d[\"offset\"] = \"{}-{}\".format(offset_from, offset_to)             \n",
    "            last_type = \"drug\"                  \n",
    "        \n",
    "        #TODO: si la paraula seguent tambe te el mateix tipus, merge\n",
    "        elif word.lower() == \"acid\":\n",
    "            prev_word, prev_offset_from, _ = tokenized_list[i-1]\n",
    "            \n",
    "            # Remove drug or brand if it was added since the next word is acid\n",
    "            if last_type != None:\n",
    "                entitites_list.pop(-1)            \n",
    "            d[\"name\"] = prev_word + \" acid\"\n",
    "            d[\"type\"] = \"drug\"\n",
    "            d[\"offset\"] = \"{}-{}\".format(prev_offset_from, offset_to)\n",
    "            last_type = \"drug\"\n",
    "    \n",
    "        else:\n",
    "            last_type = None\n",
    "        \n",
    "        # If this word was ending with , or : next word not mergerd with this one\n",
    "        if word_stripped:\n",
    "            last_type = None\n",
    "            word_stripped = False\n",
    "            \n",
    "        if d.keys(): entities_list.append(d)\n",
    "    return entities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_entities(sid, entities, outputfile):\n",
    "    for ent in entities:\n",
    "        line = '|'.join([sid, ent[\"offset\"], ent[\"name\"], ent[\"type\"]])\n",
    "        outputfile.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputdir, outputfile):\n",
    "    os.system(\"java -jar eval/evaluateNER.jar \"+ inputdir + \" \" + outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### VARIABLES\n",
    "inputdir = \"./data/Devel\"\n",
    "outputfilename=\"./output.txt\"\n",
    "outputfile = open(outputfilename, \"w\")\n",
    "\n",
    "for filename in os.listdir(inputdir):\n",
    "    \n",
    "    file_path = os.path.join(inputdir, filename)  \n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for child in root:\n",
    "        (sid, text) = (child.attrib[\"id\"], child.attrib[\"text\"])\n",
    "        span_generator = WhitespaceTokenizer().span_tokenize(text)\n",
    "        tokenized_list = tokenize(text, span_generator)\n",
    "        entities = extract_entities(tokenized_list)\n",
    "        output_entities(sid, entities, outputfile)\n",
    "\n",
    "evaluate(inputdir, outputfilename)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
