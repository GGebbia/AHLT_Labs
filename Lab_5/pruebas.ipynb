{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (probability.py, line 333)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/home/ggebbia/.local/lib/python2.7/site-packages/nltk/probability.py\"\u001b[0;36m, line \u001b[0;32m333\u001b[0m\n\u001b[0;31m    print(\"%*s\" % (width, samples[i]), end=\" \")\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (probability.py, line 333)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/home/ggebbia/.local/lib/python2.7/site-packages/nltk/probability.py\"\u001b[0;36m, line \u001b[0;32m333\u001b[0m\n\u001b[0;31m    print(\"%*s\" % (width, samples[i]), end=\" \")\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "\n",
    "# TODO see if is used\n",
    "nltk.download('punkt')\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, word, offset_from, offset_to):\n",
    "        self.word = word\n",
    "        self.offset_from = int(offset_from)\n",
    "        self.offset_to = int(offset_to)\n",
    "        self.type = \"O\"  # Initialize all tokens with type O as non-drugs non-brands and non-groups\n",
    "\n",
    "    def word_iscapitalized(self):\n",
    "        return self.word[0].isupper()\n",
    "\n",
    "    def word_isupper(self):\n",
    "        return self.word.isupper()\n",
    "\n",
    "\n",
    "class Entity:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.word = kwargs[\"text\"]\n",
    "        self.offset_from, self.offset_to = self.parse_offset(kwargs[\"charOffset\"])\n",
    "        self.type = kwargs[\"type\"]\n",
    "\n",
    "    def parse_offset(self, offset):\n",
    "\n",
    "        # offset can be given in two ways\n",
    "        # e.g.:\n",
    "        #       * 9-23\n",
    "        #       * 9-11;12-20;21-23\n",
    "        #\n",
    "        # We differenciate both cases and always save the first one and the last one\n",
    "\n",
    "        if \";\" in offset:\n",
    "            offset = offset.split(\";\")\n",
    "            offset_from = offset[0].split('-')[0]\n",
    "            offset_to = offset[-1].split('-')[1]\n",
    "        else:\n",
    "            offset_from, offset_to = offset.split('-')\n",
    "\n",
    "        return int(offset_from), int(offset_to)\n",
    "\n",
    "\n",
    "# Store as a list of dictionaries the word, the offset interval and the label (drug, group, brand, drug_n,...) of each entity in the sentence.\n",
    "def get_entities(sentence):\n",
    "    entities = []\n",
    "    for ent in sentence.findall('entity'):\n",
    "        entity = Entity(**ent.attrib)\n",
    "        entities.append(entity)\n",
    "    return entities\n",
    "\n",
    "def tokenize(sentence, entities):\n",
    "    span_generator = WhitespaceTokenizer().span_tokenize(sentence)\n",
    "    tokens = [(sentence[span[0]: span[1]], span[0], span[1] - 1) for span in span_generator]\n",
    "\n",
    "    new_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        word, offset_from, offset_to = token\n",
    "\n",
    "        if (len(word) > 1) and (word.endswith(',') or word.endswith('.') or word.endswith(':') or word.endswith(';')):\n",
    "            punct = word[-1]\n",
    "            punct_offset_from = offset_to\n",
    "            punct_offset_to = offset_to\n",
    "\n",
    "            word = word[:-1]\n",
    "            offset_to -= 1\n",
    "\n",
    "            new_tokens.append(Token(word, offset_from, offset_to))\n",
    "            new_tokens.append(Token(punct, punct_offset_from, punct_offset_to))\n",
    "\n",
    "        elif (len(word) > 1) and word[0] == '(' and (word[0:2] != '(+' or word[0:2] != '(-'):\n",
    "            punct = word[0]\n",
    "            punct_offset_from = offset_from\n",
    "            punct_offset_to = offset_from\n",
    "\n",
    "            word = word[1:]\n",
    "            offset_from += 1\n",
    "\n",
    "            new_tokens.append(Token(punct, punct_offset_from, punct_offset_to))\n",
    "            new_tokens.append(Token(word, offset_from, offset_to))\n",
    "        else:\n",
    "            new_tokens.append(Token(word, offset_from, offset_to))\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def detect_label(token, entities):\n",
    "    for entity in entities:\n",
    "        # If the two offsets are equal, then it corresponds to the same word and type.\n",
    "        if token.offset_from == entity.offset_from and token.offset_to == entity.offset_to:\n",
    "            token.type = \"B-\" + entity.type\n",
    "            return\n",
    "        # If the token offset interval is inside the entity offset interval, then it is a first or the continuation of a type sequence.\n",
    "        elif entity.offset_from <= token.offset_from and token.offset_to <= entity.offset_to:\n",
    "            if entity.offset_from == token.offset_from:\n",
    "                token.type = \"B-\" + entity.type\n",
    "            else:\n",
    "                token.type = \"I-\" + entity.type\n",
    "\n",
    "            return\n",
    "        else:\n",
    "            token.type = \"O\"\n",
    "\n",
    "def load_data(datadir):\n",
    "    #TODO\n",
    "    # returns dataset as a dict of examples: where the keys are the identities and the value is the tokenization list of the entity.\n",
    "\n",
    "    dataset_dict = {}\n",
    "    for filename in os.listdir(datadir):\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        file_path = os.path.join(datadir, filename)\n",
    "        tree = ET.parse(file_path)\n",
    "        sentences = tree.findall(\"sentence\")\n",
    "        for sentence in sentences:\n",
    "            (sid, stext) = (sentence.attrib[\"id\"], sentence.attrib[\"text\"])\n",
    "            if not stext:\n",
    "                continue\n",
    "            entities = get_entities(sentence)            \n",
    "            tokenized_sentence = tokenize(stext)\n",
    "            dataset_dict[sid] = tokenized_sentence\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "def create_indexs(datadir, max_length):\n",
    "    '''\n",
    "    Returns a mapping of each word seen in the data with an integer. Also a mapping of each tag (null, mechanism, advise,\n",
    "    effect, int). Also returns maxlen\n",
    "    It has an <UNK> token and <PAD> token that will be used for filling the encoded sentence till max_length\n",
    "    '''\n",
    "    tags = ['null', 'mechanism', 'advise', 'effect', 'int']\n",
    "    all_indexes = {}\n",
    "    word_indexes = {}\n",
    "    tags_indexes = dict(enumerate(tags))\n",
    "    all_indexes['maxlen'] = max_length\n",
    "    all_indexes['tags'] = tags_indexes\n",
    "\n",
    "\n",
    "\n",
    "def learn (traindir, validationdir, modelname):\n",
    "    '''\n",
    "    learns a NN model using traindir as training data , and validationdir\n",
    "    as validation data . Saves learnt model in a file named modelname\n",
    "    '''\n",
    "    # load train and validation data in a suitable form\n",
    "    traindata = load_data(traindir)\n",
    "    valdata = load_data(validationdir)\n",
    "\n",
    "    # create indexes from training data\n",
    "    max_len = 100\n",
    "    idx = create_indexs(traindata, max_len)\n",
    "\n",
    "    # build network\n",
    "    model = build_network(idx)\n",
    "\n",
    "    # encode datasets\n",
    "    Xtrain = encode_words(traindata,idx)\n",
    "    Ytrain = encode_tags(traindata, idx)\n",
    "    Xval = encode_words(valdata, idx)\n",
    "    Yval = encode_tags(valdata, idx)\n",
    "\n",
    "    # train model\n",
    "    model.fit(Xtrain, Ytrain, validation_data=(Xval, Yval))\n",
    "\n",
    "    # save model and indexs , for later use in prediction\n",
    "    save_model_and_indexs(model, idx, modelname)\n",
    "\n",
    "\n",
    "### MAIN ###\n",
    "data_loaded = load_data(\"data/Train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
